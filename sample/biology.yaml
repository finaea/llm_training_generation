GPT_CONFIG_124M:
  context_length: 1024
  drop_rate: 0.1
  emb_dim: 768
  n_heads: 12
  n_layers: 12
  qkv_bias: false
  vocab_size: 50257
OTHER_SETTINGS:
  batch_size: 2
  learning_rate: 0.0005
  num_epochs: 10
  train_ratio: 0.9
  weight_decay: 0.1
